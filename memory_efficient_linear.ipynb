{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Efficient Linear Layer Implementation\n",
    "\n",
    "This notebook implements a memory-efficient linear layer that processes data in batches to reduce VRAM usage during forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Tuple, Any\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def split_into_batches(tensor: torch.Tensor, batch_size: int):\n",
    "    \"\"\"Split a tensor into smaller batches.\"\"\"\n",
    "    return torch.split(tensor, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MemoryEfficientLinear(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, X: torch.Tensor, linear: nn.Linear, \n",
    "               labels: torch.Tensor, transform_fn: Callable,\n",
    "               batch_size: int = 2) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass that processes data in batches to save memory.\n",
    "        \n",
    "        Args:\n",
    "            X: Input tensor\n",
    "            linear: Linear layer\n",
    "            labels: Target labels\n",
    "            transform_fn: Function to transform outputs (e.g., cross entropy)\n",
    "            batch_size: Size of mini-batches for processing\n",
    "        \"\"\"\n",
    "        # Save tensors needed for backward pass\n",
    "        ctx.save_for_backward(X, labels)\n",
    "        ctx.linear = linear\n",
    "        ctx.transform_fn = transform_fn\n",
    "        ctx.batch_size = batch_size\n",
    "        \n",
    "        # Split input into smaller batches\n",
    "        batches = split_into_batches(X, batch_size)\n",
    "        label_batches = split_into_batches(labels, batch_size)\n",
    "        \n",
    "        # Process each batch and accumulate results\n",
    "        total_loss = 0\n",
    "        for batch, batch_labels in zip(batches, label_batches):\n",
    "            # Forward pass for current batch\n",
    "            loss = transform_fn(batch, linear, batch_labels)\n",
    "            total_loss += loss * len(batch)  # Scale by batch size\n",
    "            \n",
    "        # Average the loss\n",
    "        return total_loss / len(X)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> Tuple[torch.Tensor, None, None, None, None]:\n",
    "        \"\"\"Backward pass that computes gradients in a memory-efficient way.\"\"\"\n",
    "        X, labels = ctx.saved_tensors\n",
    "        linear = ctx.linear\n",
    "        transform_fn = ctx.transform_fn\n",
    "        batch_size = ctx.batch_size\n",
    "        \n",
    "        # Split into batches\n",
    "        batches = split_into_batches(X, batch_size)\n",
    "        label_batches = split_into_batches(labels, batch_size)\n",
    "        \n",
    "        # Initialize gradient accumulators\n",
    "        grad_X = torch.zeros_like(X)\n",
    "        grad_weight = torch.zeros_like(linear.weight)\n",
    "        grad_bias = torch.zeros_like(linear.bias) if linear.bias is not None else None\n",
    "        \n",
    "        # Process each batch\n",
    "        for i, (batch, batch_labels) in enumerate(zip(batches, label_batches)):\n",
    "            # Enable grad tracking for this batch\n",
    "            batch_tensor = batch.detach().requires_grad_()\n",
    "            \n",
    "            # Forward pass with grad tracking\n",
    "            with torch.enable_grad():\n",
    "                loss = transform_fn(batch_tensor, linear, batch_labels)\n",
    "                \n",
    "            # Backward pass for this batch\n",
    "            batch_grad = torch.autograd.grad(\n",
    "                loss, \n",
    "                [batch_tensor, linear.weight, linear.bias] if linear.bias is not None else [batch_tensor, linear.weight],\n",
    "                grad_output\n",
    "            )\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = start_idx + len(batch)\n",
    "            grad_X[start_idx:end_idx] = batch_grad[0]\n",
    "            grad_weight += batch_grad[1]\n",
    "            if linear.bias is not None:\n",
    "                grad_bias += batch_grad[2]\n",
    "        \n",
    "        return grad_X, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example transformation functions\n",
    "def cross_entropy_transform(batch: torch.Tensor, linear: nn.Linear, labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Cross entropy loss transformation.\"\"\"\n",
    "    logits = linear(batch).float()\n",
    "    return F.cross_entropy(logits.view(-1, logits.shape[-1]), labels.view(-1), reduction='mean')\n",
    "\n",
    "def mse_transform(batch: torch.Tensor, linear: nn.Linear, labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Mean squared error transformation.\"\"\"\n",
    "    output = linear(batch).float()\n",
    "    return F.mse_loss(output, labels, reduction='mean')\n",
    "\n",
    "def custom_transform(batch: torch.Tensor, linear: nn.Linear, labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Custom transformation example (L1 loss with scaling).\"\"\"\n",
    "    output = linear(batch).float()\n",
    "    return F.l1_loss(output, labels, reduction='mean') * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the implementation\n",
    "def test_memory_efficient_linear():\n",
    "    # Parameters\n",
    "    batch_size = 4\n",
    "    seq_len = 512\n",
    "    hidden_dim = 1024\n",
    "    vocab_size = 32000  # Smaller for testing\n",
    "    \n",
    "    # Create test data\n",
    "    X = torch.randn(batch_size, seq_len, hidden_dim, device='cuda')\n",
    "    labels = torch.randint(0, vocab_size, (batch_size, seq_len), device='cuda')\n",
    "    \n",
    "    # Create linear layer\n",
    "    linear = nn.Linear(hidden_dim, vocab_size, bias=True).cuda()\n",
    "    \n",
    "    # Test with different transformation functions\n",
    "    transforms = {\n",
    "        'cross_entropy': cross_entropy_transform,\n",
    "        'mse': mse_transform,\n",
    "        'custom': custom_transform\n",
    "    }\n",
    "    \n",
    "    for name, transform_fn in transforms.items():\n",
    "        print(f\"\\nTesting {name} transformation:\")\n",
    "        \n",
    "        # Memory efficient forward pass\n",
    "        efficient_output = MemoryEfficientLinear.apply(X, linear, labels, transform_fn, 2)\n",
    "        \n",
    "        # Compute gradients\n",
    "        efficient_output.backward()\n",
    "        \n",
    "        print(f\"Output shape: {efficient_output.shape}\")\n",
    "        print(f\"Output value: {efficient_output.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_memory_efficient_linear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
